---
name: vercel-tool-reduction
description: Vercel 的案例研究。他们移除了智能体 80% 的专业工具，取而代之的是单一的文件系统智能体工具，最终实现了 100% 的成功率并提升了性能。
doc_type: blog
source_url: https://vercel.com/blog/we-removed-80-percent-of-our-agents-tools
---

# 我们删掉了智能体 80% 的工具

Andrew Qu，Vercel 软件主管 | 2025 年 12 月 22 日

**结果变好了。**

我们曾花了数月时间构建了一个复杂的内部 Text-to-SQL 智能体 `d0`。它拥有精密的专业工具、沉重的提示词工程和精心的上下文管理。它能工作……某种程度上。但它很脆弱、缓慢且需要持续维护。

于是我们尝试了些不同的东西：我们删掉了大部分代码，将智能体简化到只有一个工具：**执行任意 Bash 命令**。我们称之为“文件系统智能体（File System Agent）”。Claude 直接访问你的文件，并使用 `grep`, `cat` 和 `ls` 自己搞清楚状况。

智能体在变简单的同时性能也变强了。**成功率从 80% 提升到了 100%**。更少的步骤、更少的 token、更快的响应速度。通过做减法，我们得到了一切。

---

## 什么是 d0
如果说 `v0` 是我们用于构建 UI 的 AI，那么 `d0` 就是我们用于理解数据的 AI。

`d0` 让团队中的任何人都可以通过在 Slack 中提问来做出数据驱动的决策。它将自然语言问题转化为针对我们分析基础设施的 SQL 查询。

当 `d0` 表现出色时，它能让全公司的数据访问民主化。当它出问题时，人们就会失去信任，转而继续在 Slack 上私聊分析师。我们需要 `d0` 快速、准确且可靠。

---

## 别挡模型的路
回过头来看，我们当时是在解决模型本可以自行处理的问题。我们假设它会在复杂的 Schema 中迷失、做出错误的 Join 或幻觉表名。所以我们构建了护栏：预过滤上下文、限制它的选项，并给每次交互包装上验证逻辑。我们实际上是在替模型思考：

- 构建了多个专业工具（Schema 查找、查询验证、错误恢复等）。
- 添加了沉重的提示词工程来约束推理。
- 通过精心管理上下文来避免模型过载。
- 编写了手工编码的检索逻辑来推送“相关的”Schema 信息。

每一个边缘情况都意味着一个补丁，每一次模型更新都意味着需要重新校准我们的约束。我们花在维护支架上的时间比改进智能体本身的时间还多。

---

## 一个新想法：如果我们干脆……停手呢？
我们意识到我们是在与引力对抗：约束模型的推理；总结它本可以自己阅读的信息；构建工具以防止它接触它本能处理的复杂性。

所以我们停手了。我们的假设是：如果我们直接把原始的 Cube DSL 文件（数据层定义文件）交给 Claude，让它自己“折腾”会怎样？如果 Bash 就是你所需的一切呢？随着模型变得越来越聪明、上下文窗口越来越大，也许最好的智能体架构就是几乎没有架构。

---

## v2：文件系统即智能体
新的技术栈：
- **模型**：通过 AI SDK 调用 Claude Opus 4.5。
- **执行**：用于上下文探索的 Vercel Sandbox。
- **数据层**：Cube 语义层，表现为一个包含 YAML、Markdown 和 JSON 文件的目录。

现在的“文件系统智能体”像人类分析师一样浏览我们的语义层。它读取文件、用 `grep` 查找模式、建立心智模型，并使用 `grep`, `cat`, `find` 和 `ls` 等标准 Unix 工具编写 SQL。

这之所以奏效，是因为我们的语义层本身已经是极好的文档。文件包含了维度定义、度量计算和关联关系。我们之前是在构建工具来总结那些本已清晰可见的内容。Claude 只需要直接读取它们的权限。

---

## 3.5 倍加速，减少 37% Token，100% 成功率
我们针对 5 个代表性查询对新旧架构进行了基准测试：

| 指标 | 高级版（旧） | 文件系统版（新） | 变化 |
| :--- | :--- | :--- | :--- |
| 平均执行时间 | 274.8s | 77.4s | **快 3.5 倍** |
| 成功率 | 4/5 (80%) | 5/5 (100%) | **+20%** |
| 平均 Token 使用 | ~10.2万 | ~6.1万 | **节省 37%** |
| 平均步骤 | ~12 步 | ~7 步 | **减少 42%** |

文件系统智能体在每一项对比中都胜出了。旧架构最糟糕的情况是在查询 2 中，在失败前花了 724 秒、执行了 100 步并消耗了 14.5 万 token。而文件系统智能体在 141 秒内、仅用 19 步和 6.7 万 token 就成功完成了任务。

这种质的转变同样重要。智能体捕捉到了我们从未预料到的边缘情况，并以我们可以理解的方式解释了其推理过程。

---

## 经验教训
1. **不要对抗引力**。文件系统是一种极其强大的抽象。`grep` 已经 50 岁了，但它依然能精准完成我们的需求。我们曾为了 Unix 工具早已解决的问题构建自定义工具。
2. **我们约束推理是因为我们不信任模型能推理**。对于 Opus 4.5，这种约束反而成了累赘。当我们停止为模型做决定时，它能做出更好的决定。
3. **这之所以奏效，是因为我们的语义层文档已经足够好**。YAML 文件结构良好、命名一致且定义清晰。如果你的数据层充满了糟糕的遗留命名规范且没有文档，直接给 Claude 文件权限也救不了你。
4. **“通过做减法实现加法”是真实存在的**。最好的智能体可能是工具最少的那个。每一个工具都是你在替模型做出的选择。

---

## 对智能体构建者的启示
总是会有诱惑去考虑每一种可能性。请抵制它。
从最简单的架构开始：**模型 + 文件系统 + 目标**。只有证明了必要性，才去增加复杂性。

但简单的架构本身是不够的。模型需要高质量的上下文。请在文档、清晰的命名和结构良好的数据上进行投入。这种基础建设比聪明的工具设计更重要。

模型的进化速度比你的工具更新速度还要快。**请为六个月后的模型构建，而不是为今天的。**
