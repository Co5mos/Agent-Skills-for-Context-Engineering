---
name: context-compression-evaluation
description: 评估 AI 智能体不同上下文压缩策略保留信息能力的框架，对比了结构化摘要与 OpenAI 及 Anthropic 的方案。
doc_type: research
source_url: No
---

# 评估 AI 智能体的上下文工程压缩效果

Factory Research —— 2025 年 12 月 16 日

我们构建了一个评估框架，用于衡量不同压缩策略保留上下文信息的程度。通过在现实世界中长周期的智能体 session（涵盖调试、代码审查和功能实现）上测试三种方法，我们发现：**结构化摘要**比 OpenAI 和 Anthropic 的替代方案能保留更多有用信息。

---

## 01 问题背景
长周期的智能体 session 可能产生数百万 token 的对话历史，这远超任何模型的工作内存。

天真的解决方案是强力压缩：将一切挤进尽可能小的摘要中。但这增加了智能体忘记已修改文件或已尝试方法的概率。这会导致智能体浪费 token 重新读取文件和重新探索死胡同。

正确的优化目标不是“每次请求的 token 数”，而是**“完成每项任务的 token 数”**。

## 02 衡量上下文质量
传统的 ROUGE 或嵌入相似度指标无法告诉你智能体在压缩后能否继续有效工作。

我们设计了一种基于“探测（probe）”的评估方法，直接衡量功能质量。做法很简单：在压缩后，向智能体提出需要记住截断历史中特定细节的问题。如果压缩保留了正确信息，智能体能正确回答；否则，它会瞎猜或产生幻觉。

我们使用四种探测类型：
- **回忆 (Recall)**：事实保留。“原始错误消息是什么？”
- **制品 (Artifact)**：文件追踪。“我们修改了哪些文件？描述每个文件的变化。”
- **延续 (Continuation)**：任务规划。“我们下一步应该做什么？”
- **决策 (Decision)**：推理链。“我们讨论了 Redis 问题的选项。最后决定了什么？”

## 03 三种压缩方法
我们对比了三种生产级的压缩策略：

1.  **Factory (锚定迭代摘要)**：维护一个结构化的持久摘要，包含显式的分区（会话意图、文件修改、已做决策、下一步）。触发压缩时，仅对新截断的部分进行摘要，并合并到现有摘要中。结构化强制了信息的保留，防止文件路径或决策被无声丢弃。
2.  **OpenAI (/responses/compact)**：产生不透明的、经过高度压缩的表示。其压缩率最高（99.3%），但牺牲了可读性和解释性。
3.  **Anthropic (Claude SDK 内置压缩)**：产生详细、结构化的摘要。与 Factory 的不同点在于其更新机制：Anthropic 在每次压缩时重新生成完整摘要，而 Factory 采用锚定方式增量合并。

## 04 实验结果
我们在包含 PR 审查、测试、Bug 修复、功能实现和重构的 36,000 条生产消息上进行了评估。

| 方法 | 综合评分 | 准确率 | 上下文 | 制品追踪 | 完整性 | 延续性 | 指令遵循 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **Factory** | **3.70** | **4.04** | **4.01** | **2.45** | **4.44** | **3.80** | **4.99** |
| Anthropic | 3.44 | 3.74 | 3.56 | 2.33 | 4.37 | 3.67 | 4.95 |
| OpenAI | 3.35 | 3.43 | 3.64 | 2.19 | 4.37 | 3.77 | 4.92 |

*Factory 在各维度上均领先。OpenAI 的压缩率虽然最高，但在质量评分上落后了 0.35 分。这些遗失的细节最终会导致重复读取，从而抵消节省的 token。*

## 05 核心发现
1.  **结构至关重要**：通用摘要会将所有内容视为同等可压缩。文件路径在信息论上可能“低熵”，但对智能体来说是核心信息。强制摘要器填充显式的分区（文件、决策、下一步）可以防止信息漂移。
2.  **压缩率是错误的指标**：OpenAI 实现了 99.3% 的压缩率，但质量评分较低。对于重读成本高昂的任务，Factory 的这种权衡（保留多一点 token，提高质量）更有利。
3.  **制品追踪（Artifact tracking）仍是难题**：所有方法在“了解创建/修改/检查了哪些文件”方面的得分都在 2.19 到 2.45 之间（满分 5.0）。这表明制品保留需要摘要之外的专门处理（如单独的制品索引）。
4.  **基于探测的评估更有效**：传统的 ROUGE 指标衡量词汇相似度，而我们的方法衡量摘要是否真正能让任务延续。

## 06 附录：评估维度说明
- **准确性 (Accuracy)**：事实、文件路径和技术细节是否正确？
- **上下文感知 (Context Awareness)**：响应是否反映了当前的对话状态和制品状态？
- **制品链完整性 (Artifact Trail)**：智能体是否知道哪些文件被创建/修改，以及具体的变更细节？
- **延续性保留 (Continuity)**：智能体能否不重新读取信息就继续工作？是否记得待办事项和决策背后的理由？
- **完整性 (Completeness)**：响应是否涵盖了问题的各部分？深度是否足够？
- **指令遵循 (Instruction Following)**：是否遵守了要求的格式和约束？
