# 案例研究：LLM 项目开发 (Case Studies: LLM Project Development)

本参考文档包含生产环境中 LLM 项目的详细案例研究，展示了有效的开发方法。

## 案例 1：Karpathy 的 HN 时间胶囊 (HN Time Capsule)

**目标**：分析 10 年前的 Hacker News 讨论，并根据事后之明对评论者的预见性进行评分。

### 核心开发步骤
1.  **手动原型验证**：Karpathy 在写任何代码前，先通过手动把一个帖子复制到 ChatGPT 中来测试可行性。这耗时几分钟，但确认了模型具备洞察力且预想的格式可行。
2.  **助手辅助实现**：利用 Opus 4.5 模型在约 3 小时内完成了整个流水线的搭建，包括 HTML 解析、API 集成和渲染。
3.  **流水线架构**：采用了 `获取 (fetch) → 准备 (prompt) → 分析 (analyze) → 解析 (parse) → 渲染 (render)` 的五阶段设计。

### 经验教训
-   **文件系统即状态**：每个文章目录包含所有中间输出，调试极其简单。
-   **幂等阶段**：重跑脚本时只会处理缺少输出文件的项目。
-   **并行执行**：使用 15 个并行 worker 缩短了时间，且不增加 token 成本。

## 案例 2：Vercel d0 架构减法

**目标**：构建一个 text-to-SQL 助手。

### 初始尝试（失败）
团队构建了一个拥有 17 个专门工具（架构查找、查询验证、错误恢复等）的复杂系统。
- **结果**：80% 成功率，平均执行耗时 274.8 秒。

### 架构减法（成功）
**假设**：如果只给模型 raw files 的访问权限，让它自己想办法呢？
- **新架构**：总共只有 2 个工具：Bash (ExecuteCommand) 和 ExecuteSQL。
- **结果**：100% 成功率，平均执行耗时 77.4 秒（提速 3.5 倍），Token 使用减少 37%。

### 核心洞察
-   **减法即加法**：最好的助手可能是工具最少的助手。每个工具其实是开发者替模型做出的选择，而更好的模型不需要这些限制。
-   **文档优于昂贵的工具**：将精力花在完善文档、清晰命名和结构化数据上，比构建复杂的支架更重要。

## 案例 3：Manus 上下文工程经验

**核心洞察**：**KV-cache 命中率**是生产级助手最重要的指标，直接影响延迟和成本（Claude Sonnet 命中缓存与否有 10 倍的成本差距）。

### 关键模式
1.  **Append-only 上下文**：永远不要修改之前的动作或观察结果。保持确定性的序列化（如 JSON key 顺序稳定），哪怕一个 token 的差异也会导致缓存失效。
2.  **遮蔽（Mask）而非移除**：不要在中途添加或移除工具定义。使用 logit masking 来约束工具选择，否则会破坏 KV-cache。
3.  **文件系统即上下文**：将文件系统视为无限的、持久的、助手可操作的记忆。
4.  **诵读以唤起注意力 (Recitation)**：助手不断重写 `todo.md`，将全局计划推入模型的“最近注意力跨度”，避免“迷失在中间”带来的目标漂移。

## 案例 4：Anthropic 多助手研究系统

**核心洞察**：三个因素解释了 95% 的性能差异：Token 使用量（贡献了 80%）、工具调用数量和模型选择。多助手架构本质上是通过增加 token 投入来扩展推理上限。

### 提示词原则
-   **教导委派 (Delegation)**：子助手需要明确的目标、输出格式、工具和清晰的边界。
-   **让助手自我提升**：让 Claude 分析提示词失败的原因并提出改进建议。
-   **并行工具调用**：并行运行 3-5 个子助手，可将研究时间缩短多达 90%。

## 跨案例总结

### 成功的共同因素
1.  **自动化前先手动验证**。
2.  **文件系统是基石**：无论是用于状态管理、工具接口还是记忆。
3.  **架构简单化**：减法往往胜过加法。
4.  **结构化输出与鲁棒解析**：明确格式规范 + 容错解析。

### 常见的失败点
1.  **过度约束模型**：当初为了弱模型设计的“由于恐惧而加的限制”成为了强模型的阻碍。
2.  **工具泛滥**：工具越多，往往意味着更多的混淆。
3.  **隐藏错误**：将失败从上下文中擦除，剥夺了模型学习和校准的机会。
