---
name: context-optimization
description: 当用户要求“优化上下文”、“降低 Token 成本”、“提高上下文效率”、“实现 KV 缓存优化”、“划分上下文”，或提到上下文限制、观察遮蔽、上下文预算或扩展有效上下文容量时，应使用此技能。
---

# 上下文优化技术

上下文优化通过战略性的压缩、遮蔽（masking）、缓存和分区，扩展了有限上下文窗口的有效容量。其目标不是魔术般地增加上下文窗口，而是更好地利用现有容量。有效的优化可以在不需要更大模型或更长上下文的情况下，将有效上下文容量增加两倍甚至三倍。

## 何时激活

当出现以下情况时激活此技能：
- 上下文限制约束了任务复杂度。
- 旨在降低成本（更少的 Token = 更低的成本）。
- 需要降低长对话的延迟。
- 实现长期运行的智能体系统。
- 需要处理更大的文档或更长的对话。
- 构建大规模生产系统。

## 核心概念

上下文优化通过四种主要策略扩展有效容量：**压缩**（在接近限制时总结上下文）、**观察遮蔽**（用引用替换冗长的输出）、**KV 缓存优化**（重用已缓存的计算结果）以及**上下文分区**（将工作拆分到隔离的上下文中）。

关键见解在于，上下文的“质”比“量”更重要。优化可以在减少噪音的同时保留信号。其艺术在于选择保留什么、丢弃什么，以及何时应用每种技术。

## 详细主题

### 压缩策略

**什么是压缩**
压缩是指在接近限制时总结上下文内容，然后使用摘要重新初始化一个新的上下文窗口。这以高保真度提炼了上下文窗口的内容，使智能体能够继续工作且性能下降微乎其微。

压缩通常是上下文优化的第一手段。其艺术在于选择保留什么与丢弃什么。

**压缩实现**
压缩的工作原理是识别可以压缩的部分，生成捕获要点的摘要，并用摘要替换完整内容。压缩的优先级：工具输出（替换为摘要）> 旧的回合（总结早期对话）> 检索到的文档（如果有近期版本则总结），**严禁压缩系统提示词**。

**摘要生成**
有效的摘要根据消息类型保留不同的元素：

- 工具输出：保留关键发现、指标和结论。移除冗长的原始输出。
- 对话回合：保留关键决策、承诺和上下文转移。移除填充词和往复过程。
- 检索到的文档：保留关键事实和声明。移除支持性证据和详细阐述。

### 观察遮蔽 (Observation Masking)

**观察结果带来的问题**
在智能体轨迹中，工具输出可能占到 Token 使用量的 80% 以上。其中大部分是已经完成使命的冗长输出。一旦智能体利用工具输出做出了决策，保留完整输出的价值就会递减，同时消耗大量上下文。

观察遮蔽用简洁的引用替换冗长的工具输出。如果需要，信息仍然可以访问，但不会持续消耗上下文。

**遮蔽策略选择**
并非所有观察结果都应被同等遮蔽：

- 绝对不遮蔽：对当前任务至关重要的观察、最近一轮的观察、活跃推理中使用的观察。
- 考虑遮蔽：3 轮以前的观察、可以提取关键点但输出冗长的观察、使命已达成效果的观察。
- 总是遮蔽：重复的输出、样板式的页眉/页脚、已经在对话中总结过的输出。

### KV 缓存优化 (KV-Cache Optimization)

**理解 KV 缓存**
KV 缓存存储了推理期间计算的键（Key）和值（Value）张量，随序列长度线性增长。跨共享相同前缀的请求缓存 KV 缓存可以避免重复计算。

前缀缓存利用基于哈希的块匹配，在具有相同前缀的请求间重用 KV 块。这极大地降低了系统提示词等公共前缀请求的成本和延迟。

**缓存优化模式**
通过重新对上下文元素排序来最大化缓存命中，从而进行优化。将稳定的元素放在首位（系统提示词、工具定义），然后是频繁重用的元素，最后是独特的元素。

设计提示词以最大化缓存稳定性：避免使用时间戳等动态内容，使用一致的格式，保持跨会话结构的稳定。

### 上下文分区 (Context Partitioning)

**子智能体分区**
上下文优化最激进的形式是将 work 划分给具有隔离上下文的子智能体。每个子智能体在一个专注于其子任务的干净上下文中运行，不携带来自其他子任务的累积上下文。

这种方法实现了关注点分离——详细的搜索上下文保留在子智能体内部，而协调者专注于综合和分析。

**结果聚合**
通过验证所有分区是否完成、合并兼容结果并在仍然过大时进行总结，来聚合来自分区子任务的结果。

### 预算管理

**上下文预算分配**
设计显式的上下文预算。将 Token 分配到各类别：系统提示词、工具定义、检索到的文档、消息历史记录和预留缓冲区。对照预算监控使用情况，并在接近限制时触发优化。

**基于触发器的优化**
监控优化触发器的信号：Token 利用率超过 80%、性能下降指标、响应质量下降。根据上下文组成应用适当的优化技术。

## 实践指南

### 优化决策框架

何时优化：
- 上下文利用率超过 70%
- 随着对话延长，响应质量下降
- 由于上下文过长导致成本增加
- 延迟随对话长度增加

应用什么：
- 工具输出占主导：观察遮蔽
- 检索文档占主导：总结或分区
- 消息历史占主导：带总结的压缩
- 多个组件：组合策略

### 性能考量

压缩应实现 50-70% 的 Token 减少，且质量下降小于 5%。遮蔽应在被遮蔽的观察结果中实现 60-80% 的减少。对于稳定的工作负载，缓存优化应实现 70% 以上的命中率。

根据测量的有效性监控并迭代优化策略。

## 示例

**示例 1：压缩触发器**
```python
if context_tokens / context_limit > 0.8:
    context = compact_context(context)
```

**示例 2：观察遮蔽**
```python
if len(observation) > max_length:
    ref_id = store_observation(observation)
    return f"[观察结果:{ref_id} 已省略。关键点：{extract_key(observation)}]"
```

**示例 3：对缓存友好的排序**
```python
# 稳定内容在前
context = [system_prompt, tool_definitions]  # 可缓存
context += [reused_templates]  # 可重用
context += [unique_content]  # 唯一
```

## 指南

1. 优化前先测量——了解当前状态。
2. 在可能的情况下，先应用压缩再应用遮蔽。
3. 通过一致的提示词设计缓存稳定性。
4. 在上下文变得成问题之前进行分区。
5. 持续监控优化的有效性。
6. 平衡 Token 节省与质量保留。
7. 在生产规模下测试优化。
8. 为边缘情况实现优雅降级。

## 集成

此技能建立在 context-fundamentals 和 context-degradation 的基础上。它连接到：
- multi-agent-patterns：作为隔离手段的分区
- evaluation：衡量优化有效性
- memory-systems：将上下文移交给记忆系统

## 参考资料

内部参考：
- [优化技术参考](./references/optimization_techniques.md) - 详细技术参考

系列相关技能：
- context-fundamentals - 上下文基础
- context-degradation - 了解何时优化
- evaluation - 衡量优化效果

外部资源：
- 上下文窗口限制的相关研究
- KV 缓存优化技术
- 生产工程指南

---

## 技能元数据

**创建日期**：2025-12-20
**上次更新**：2025-12-20
**作者**：上下文工程智能体技能贡献者
**版本**：1.0.0
