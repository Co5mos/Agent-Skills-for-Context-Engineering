---
name: advanced-evaluation
description: 当用户要求“实现 LLM 作为裁判（LLM-as-judge）”、“比较模型输出”、“创建评估量表（rubrics）”、“减轻评估偏差”，或提到直接评分、成对比较、位置偏见、评估流水线或自动化质量评估时，应使用此技巧。
---

# 高级评估 (Advanced Evaluation)

本技巧涵盖了使用 LLM 作为裁判来评估 LLM 输出的生产级技术。它综合了学术论文、行业实践和实际经验，为构建可靠的评估系统提供了可操作的模式。

**核心洞察**：**LLM-as-a-Judge（LLM 裁判）** 不是单一的技术，而是一族方法。选择合适的方法并减轻已知的偏差（biases）是这一技巧的核心能力。

## 何时激活

在以下情况下启用此技能：
- 为 LLM 输出构建自动评估流水线
- 比较多个模型响应以选择最佳响应
- 在评估团队中建立一致的质量标准
- 调试结果不一致的评估系统
- 设计提示词或模型变更的 A/B 测试
- 为人工或自动评估创建量表（rubrics）

## 核心概念

### 评估分类 (The Evaluation Taxonomy)
评估方法主要分为两类，各具特色：

1.  **直接评分 (Direct Scoring)**：单个 LLM 按定义的量表为一个响应评分。
    - **适用场景**：客观标准（事实准确性、指令遵循、格式合规）。
    - **失效模式**：评分校准漂移、量表解读不一致。
2.  **成对比较 (Pairwise Comparison)**：某个 LLM 比较两个响应并选出较优者。
    - **适用场景**：主观偏好（语气、风格、说服力）。
    - **失效模式**：位置偏见（Position bias）、长度偏见（Length bias）。

### 偏见分析 (The Bias Landscape)
LLM 裁判表现出系统性的偏见，必须主动缓解：
- **位置偏见**：成对比较中，排在第一个位置的响应通常获得更高分。
- **长度偏见**：无论质量如何，较长的响应通常得分更高。
- **自提升偏见**：模型给自己的输出打分更高。
- **权威性偏见**：自信、有张力的语气更容易获得高分，无论其准确性如何。

## 评估方法实现

### 直接评分实现
直接评分需要三个组件：清晰的标准、校准后的量表和结构化的输出。
- **思维链 (CoT) 要求**：所有评分提示词必须要求**“先给出理由，再给出分数”**。研究表明，与先给分相比，这能将可靠性提高 15-25%。

### 成对比较实现
由于位置偏见的存在，成对比较必须采用**“交换位置”协议**：
1. 第一遍：响应 A 在前，响应 B 在后。
2. 第二遍：响应 B 在前，响应 A 在后。
3. 一致性检查：如果两次结论不一致，则判定为“平局（TIE）”并降低置信度。

### 量表 (Rubric) 生成
与开放式评分相比，定义良好的量表能将评估方差降低 40-60%。
- **标准组件**：等级描述、可观察的特征描述、示例（optional）、边缘情况处理指南。

## 实践指南

### 决策框架：直接评分 vs 成对比较
1. **是否存在客观的正确答案？** -> 是：**直接评分**（如事实准确性、指令遵循）。
2. **是否属于偏好或质量判断？** -> 是：**成对比较**（如语气、风格、创造力）。

### 常见反模式 (Anti-Patterns)
- **无理由评分**：评分缺乏根据，难以调试。建议：要求基于证据的理由。
- **单次成对比较**：位置偏见会污染结果。建议：交换位置并检查一致性。
- **标准过载**：一个标准测量多件事。建议：一个标准 = 一个可测量的维度。

## Guidelines

1. **始终要求先写理由，再打分**。
2. **始终在成对比较中交换位置**。
3. **区分客观与主观标准**。
4. **显式包含置信度得分**，并根据位置一致性和证据强度进行校准。
5. **显式定义边缘情况**。
6. **使用领域特定的量表**。

## 技巧元数据

**创建日期**: 2024-12-24
**作者**: Muratcan Koylan
**版本**: 1.0.0
