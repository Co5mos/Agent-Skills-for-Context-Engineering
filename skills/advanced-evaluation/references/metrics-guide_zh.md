# LLM 评估指标选择指南 (Metric Selection Guide for LLM Evaluation)

本参考文档提供在不同评估场景中选择合适指标的指南。

## 指标类别 (Metric Categories)

### 分类指标 (Classification Metrics)
用于二元或多类评估任务（如：通过/失败、正确/错误）。
- **精确率 (Precision)**：在所有裁判判定为“好”的响应中，有多少是真的“好”？适用于误报（False Positive）成本较高的场景。
- **召回率 (Recall)**：在所有客观为“好”的响应中，裁判识别出了多少？适用于漏报（False Negative）成本较高的场景。
- **F1 分数 (F1 Score)**：精确率和召回率的调和平均数，用于平衡两者。

### 一致性指标 (Agreement Metrics)
用于衡量自动评估与人工判断之间的一致性。
- **Cohen's Kappa (κ)**：调整了偶然因素后的协议比例。
    - κ > 0.8：近乎完美的协议。
    - 0.6 - 0.8：高度一致。
- **加权 Kappa (Weighted Kappa)**：适用于等级量表，会加大对严重分歧（如 1 分与 5 分）的惩罚。

### 相关性指标 (Correlation Metrics)
用于处理等级或连续评分。
- **Spearman 秩相关系数 (ρ)**：衡量排名的相关性。ρ > 0.9 表示极强相关。适用于“顺序比绝对值更重要”的情况。
- **Kendall's Tau (τ)**：类似于 Spearman，但在处理大量相同值（Tied values）时更稳健。
- **Pearson 相关系数 (r)**：衡量评分之间的线性相关性。适用于“绝对分值很重要”的情况。

### 成对比较指标 (Pairwise Comparison Metrics)
- **协议率 (Agreement Rate)**：简单的协议百分比。
- **位置一致性 (Position Consistency)**：交换位置后，结论保持不变的比例。用于衡量位置偏见的影响。

## 指标选择决策树

1. **二元分类 (Pass/Fail)**：使用精确率、召回率、F1、Cohen's κ。
2. **等级量表 (1-5 评分)**：
    - 与人工判断比较：使用 Spearman's ρ、加权 κ。
    - 比较两名自动裁判：使用 Kendall's τ、Spearman's ρ。
3. **成对偏好**：使用协议率、位置一致性。
4. **多标签分类**：使用 Macro-F1、Micro-F1。

## 指标解读参考

| 指标 | 优秀 | 可接受 | 有问题 |
| :--- | :--- | :--- | :--- |
| **Spearman's ρ** | > 0.8 | 0.6 - 0.8 | < 0.6 |
| **Cohen's κ** | > 0.7 | 0.5 - 0.7 | < 0.5 |
| **位置一致性** | > 0.9 | 0.8 - 0.9 | < 0.8 |
| **长度相关性** | < 0.2 | 0.2 - 0.4 | > 0.4 |

## 预警信号 (Warning Signs)
1. **协议率高但相关性低**：可能存在打分范围不同的校准问题。
2. **位置一致性低**：位置偏见在干扰结果。
3. **长度相关性高**：存在明显的长度偏见。
4. **各标准间方差大**：某些标准定义不清晰，导致裁判解读困难。
