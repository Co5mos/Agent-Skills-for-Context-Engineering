# LLM 评估偏差缓解技术 (Bias Mitigation Techniques for LLM Evaluation)

本参考文档详细介绍了在 LLM 作为裁判的系统中，缓解已知偏见（biases）的具体技术。

## 位置偏见 (Position Bias)

### 问题描述
在成对比较（Pairwise comparison）中，LLM 会更偏向排在特定位置的响应。例如，GPT 通常表现出约 5% 的首位偏见。

### 缓解方案
- **位置交换协议 (Position Swapping)**：先后执行两次比较，第一次 A 在前 B 在后，第二次 B 在前 A 在后。检查两次结论是否一致。若不一致，判定为 **TIE（平局）**。
- **多次洗牌 (Multiple Shuffles)**：执行 3 次或更多次不同顺序的比较，采用**多数票（Majority vote）**原则，并以得票率作为一致性指标。

## 长度偏见 (Length Bias)

### 问题描述
LLM 倾向于给字数更多的响应打高分，即使内容冗余。

### 缓解方案
- **显式提示词提示**：在提示词中明确加入：“不要因为响应更长就偏好它”、“惩罚不必要的冗余或重复”、“关注信息密度”。
- **长度归一化评分**：根据响应长度与目标长度的比例，对最终得分进行惩罚性校准。
- **独立简洁性标准**：将“简洁性（Conciseness）”作为一个显式的评分标准（权重例如 0.3），使其不只是隐含在质量评价中。

## 自提升偏见 (Self-Enhancement Bias)

### 问题描述
模型倾向于给自己或同系列模型生成的输出打更高分。

### 缓解方案
- **交叉模型评估**：使用与生成模型不同系列的型号作为裁判。例如，用 Claude 来评估 GPT 的输出。
- **盲测 (Blind Evaluation)**：在评估前移除响应中能体现模型特征的表达（如“作为 AI”等）。

## 冗余偏见 (Verbosity Bias)

### 问题描述
详细的解释即使不相关或不正确，也往往能获得更高的评分。

### 缓解方案
- **相关性加权评分**：先评估响应各部分的相关性，再对评分进行加权。
- **带冗余惩罚的量表**：在 5 分量表中明确写明：“满分要求简洁且完整；虽完整但冗余的响应应扣分”。

## 权威性偏见 (Authority Bias)

### 问题描述
自信、带有权威语气的内容更容易获得好评，无论其是否准确。

### 缓解方案
- **强制证据要求**：要求裁判识别响应中的事实声明，并核实其是否有证据支持。明确指明：“自信但无证据的声明得分不应高于带犹豫语气但有证据的声明”。
- **事实核查层**：在评分前增加一个独立的 Fact-check 环节，根据核实比例调整最终得分。

## 汇总监测表

| 偏见 | 主要缓解手段 | 次要缓解手段 | 检测方法 |
| :--- | :--- | :--- | :--- |
| **位置** | 位置交换 | 多次洗牌 | 一致性检查 |
| **长度** | 显式提示词 | 长度归一化 | 长度-得分相关性分析 |
| **自提升** | 交叉模型评估 | 匿名化处理 | 模型比较研究 |
| **冗余** | 相关性加权评分 | 量表惩罚项 | 相关性打分 |
| **权威性** | 证据要求 | 事实核查层 | 置信度-准确度相关性分析 |
