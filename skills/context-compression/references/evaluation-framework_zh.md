# 上下文压缩评估框架 (Context Compression Evaluation Framework)

本文档提供用于衡量上下文压缩质量的完整评估框架，包括探测类型、评分标准和 LLM 裁判配置。

## 探测类型 (Probe Types)

### 回忆探测 (Recall Probes)
测试对对话历史中特定事实细节的保留情况。
- **示例**：“最初导致此调试会话的错误消息是什么？”、“我们决定使用哪个版本的依赖项？”

### 制品探测 (Artifact Probes)
测试文件追踪和修改感知。
- **示例**：“我们修改了哪些文件？描述每个文件的变化。”、“我们在本次会话中创建了哪些新文件？”

### 延续探测 (Continuation Probes)
测试在不重新获取上下文的情况下继续工作的能力。
- **示例**：“我们下一步应该做什么？”、“哪些测试仍然失败，为什么？”

### 决策探测 (Decision Probes)
测试对推理链和决策理由的保留。
- **示例**：“关于 Redis 问题，我们最终决定了什么，为什么？”、“为什么我们选择连接池而非单次请求连接？”

## 评分标准 (Scoring Rubrics)

评估分为六个维度，每个维度包含 2-3 个标准。评分采用 0-5 分制：

- **准确性 (Accuracy)**：事实、文件路径和技术细节是否正确？
- **上下文感知 (Context Awareness)**：响应是否反映了当前的对话状态和制品状态？
- **制品链 (Artifact Trail)**：是否知道哪些文件被创建/修改，以及具体的变更细节？
- **完整性 (Completeness)**：是否涵盖了问题的各部分？深度是否足够？
- **延续性 (Continuity)**：是否能不重新读取信息就继续工作？
- **指令遵循 (Instruction Following)**：是否遵守了要求的格式和约束？

## LLM 裁判配置 (LLM Judge Configuration)

### 系统提示词
```text
你是一位评估软件开发对话中 AI 助手响应的专家。
你的任务是根据特定的评分标准为响应打分。
1. 仔细阅读标准问题。
2. 检查响应中的证据。
3. 根据评分指南给出 0-5 分的分数。
4. 为你的评分提供简要理由。
要客观且一致。关注响应中呈现的内容，而非本可以包含的内容。
```

## 基准测试结果参考

| 方法 | 综合 | 准确性 | 上下文 | 制品 | 完整性 | 延续性 | 指令 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **锚定迭代式** | **3.70** | **4.04** | **4.01** | **2.45** | **4.44** | **3.80** | **4.99** |
| 再生式 | 3.44 | 3.74 | 3.56 | 2.33 | 4.37 | 3.67 | 4.95 |
| 不透明式 | 3.35 | 3.43 | 3.64 | 2.19 | 4.37 | 3.77 | 4.92 |

## 实施注意事项
- **探测生成**：在每个压缩点，根据截断的历史提取事实、文件操作、未完成任务和决策点。
- **打分流程**：将探测问题 + 模型响应 + 压缩后的上下文喂给裁判模型。
- **盲测 (Blinding)**：裁判模型不应知道哪个压缩方法产生了该响应，以防止偏见。
